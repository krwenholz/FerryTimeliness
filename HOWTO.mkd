# Training the Data #
The data obtained for weather and for the ferry system is contained in this
package (or by request).  Below are the steps to cull the data into a form
usable by LibSVM and how to run it through LibSVM.

The data used spanned October 2010 through September 2012.  A lot of cleaning
and investigation into meaning was required to make the data make sense and work.
All source files are in the 'src' directory, and any data files are expected to
be in or go in the '../Data' directory.

## Weather Data ##
### Where does it come from? ###
Quality controlled [data](http://cdo.ncdc.noaa.gov/qclcd/QCLCD?prior=N)
from NOAA was obtained with access from the University
of Puget Sound.  This data was downloaded month by month to match the ferry
data timeline.

### How does weather_data_reader.py work? ###
The weather_data_reader.py script is used to parse all of the weather data pulled
from the NOAA source into a single CSV file (header included).  To adjust this
file, all you need to do is adjust the printed header at the top (line 17) and
the tuple of column numbers (line 21) to include the columns you want.  To run
from the command line you use

    python weather_data_reader.py

The output is a file, "weather_data.csv", in the data directory (line 15).
Command line output will indicate how many points are dropped due to empty
or bad data type entries in the desired columns.


## Ferry Data ##
### Where does it come from? ###
The data is basically a copy of the information found on the
[WSDOT VesselWatch site](http://www.wsdot.com/ferries/vesselwatch/Default.aspx).
To get a copy of two years' worth (October 2010 - September 2012), I just filed
a records request.  Super easy, but it did take almost a month to actually get
the data.

### How does wsdot_data_request_reader.py work? ###
The script is designed to clean up the state data a bit: remove rows with
null entries, bad types, etc.  You'll get some console output indicating how
many good rows are found.  Almost all of the configuration happens at the
bottom of the file.  By default, expect the file to be read in from
'../Data/data_request_October2012.csv' and output to '../Data/ferry_data.csv'.
A nice header is even included.  To use, simply run

    python wsdot_data_request_reader.py

In theory, the output CSV file should be ready for a database (Postgres) using
simple inserts (i.e. you shouldn't even need to format the dates).


## Preparing Weather and Ferry Data ##
### What has to be done with? ###
We have to join the weather and ferry data if we ever want to test the usefulness
of weather in predicting ferry tardiness.  The script, for better or worse, does
more than this, though.  Most importantly, it categorizes and labels the data
by turning categorical variables into something like a bit vector appended to the
current row and then putting a 1 at the head for within three minutes on time and
-1 otherwise.  In essence, even after aggregating the data, it is necessary to
perform several cleaning operations, some of which we use LibSVM for (discussed
near the end).

### How does libsvm_data_format.py work? ###
The bottom of the file is a full setup to combine weather and ferry data (some
lines may be commented out depending on your version).  From the top down to
this script part, you'll find all of the functions for cleaning up the data:
scaling, handling categorical variables, labeling, joining, and outputting in
a format understandable by LibSVM.  To run the script, use the following

    python2 libsvm_data_format.py ../Data/<weather_data> ../Data/<ferry_data> >
        ../Data/<formatting_output>

On completion, you will find "../Data/svm_points_data.txt" and
"../Data/svm_points_test.txt" with training and testing data ready to feed to
LibSVM.  In "../Data/<formatting_output>" will be a variety of outputs
describing how the data was transformed into a suitable output.

## Using LibSVM (my procedure) ##
Several steps are needed to actually find the SVM model.  Originally, my
libsvm_data_format script was doing the subset creation for testing, but you
can remove this part easily and just use LibSVMs support (which is better
anyway).  These steps aren't necessarily the most efficient, but they work.
Below is the series of steps I ran from the command line:

    # It's easiest to just have each file you need in the libsvm directory.
    # This avoids many of the path naming and other mystery issues I encountered.
    # From the libsvm root, we start by scaling.
    $ ./svm-scale -l -1 -u 1 -s range svm_data_points > svm_data_points.scale

    # Next we make a subset of the data.
    $ cp svm_data_points.scale tools
    $ python2 subset.py svm_data_points.scale 50000 test_data train_data
    $ python2 subset.py train_data 20000 param_data
    $ mv test_data ..
    $ mv train_data ..

    # grid.py helps us find best fit parameters for gamma and c.  The -m parameter
    # provides it with more than the default 100 MB of memory.
    $ grid.py -m 900 param_data
    ...
    2408 0.00723 88
    # This would be best parameters of C=2408, gamma=0.00723, and an 88% accuracy.

    # Next is to use the best fit parameters in training.
    $ cd ..
    $ ./svm-train -c 2408 -g 0.00723 -m 900 train_data
    $ ./svm-predict test_data train_data.model test_data.predict
    ...
    Accuracy = 88.92 %

It's all pretty straightforward, but you want to have a straightforward process
to repeat if you plan on making multiple models for testing.









